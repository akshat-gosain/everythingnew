{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat_gosain/.local/lib/python3.10/site-packages/sklearn/datasets/_openml.py:1002: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of nodes: 790\n",
      "Train F1-score: 0.01801445332527013\n",
      "Train Accuracy: 0.13837498878172882\n",
      "Validation F1-score: 0.01801445332527013\n",
      "Validation Accuracy: 0.05080357688198742\n",
      "\n",
      "Number of nodes: 1024\n",
      "Train F1-score: 0.010267791598400022\n",
      "Train Accuracy: 0.16980358731966097\n",
      "Validation F1-score: 0.010267791598400022\n",
      "Validation Accuracy: 0.03244641264795954\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the MNIST dataset\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "\n",
    "# Convert the string labels to integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the activation functions and their derivatives\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=0, keepdims=True))\n",
    "    return exps / (np.sum(exps, axis=0, keepdims=True) + 1e-10)\n",
    "\n",
    "def softmax_derivative(x):\n",
    "    p = softmax(x)\n",
    "    return p * (1 - p)\n",
    "\n",
    "def hidden_activation(x, k0, k1):\n",
    "    return k0 + k1 * x\n",
    "\n",
    "def hidden_activation_derivative(x, k0, k1):\n",
    "    return k1\n",
    "\n",
    "n_i = X_train.shape[1]  # Number of inputs\n",
    "n_o = len(np.unique(y))  # Number of outputs\n",
    "\n",
    "avg_train_f1_scores = []\n",
    "\n",
    "learning_rate = 0.21\n",
    "\n",
    "hidden_layer_sizes = [790, 1024,4000]\n",
    "kf = KFold(n_splits=3)\n",
    "\n",
    "# Initialize the best scores and the corresponding number of hidden nodes\n",
    "best_train_f1 = -np.inf\n",
    "best_train_accuracy = -np.inf\n",
    "best_test_f1 = -np.inf\n",
    "best_test_accuracy = -np.inf\n",
    "best_n_h = None\n",
    "\n",
    "# Initialize lists to store the training loss and F1-score for each epoch\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# Iterate over the different numbers of nodes in the hidden layer\n",
    "for n_h in hidden_layer_sizes:\n",
    "    f1_scores = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = [] \n",
    "\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "        k0 = np.random.randn()\n",
    "        k1 = np.random.randn()\n",
    "\n",
    "        np.random.seed(42)\n",
    "        W1 = np.random.randn(n_h, n_i)\n",
    "        b1 = np.zeros(n_h)\n",
    "        W2 = np.random.randn(n_o, n_h)\n",
    "        b2 = np.zeros(n_o)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            Z1 = np.dot(W1, X_train_fold.T) + b1[:, None]\n",
    "            A1 = hidden_activation(Z1, k0, k1)\n",
    "            Z2 = np.dot(W2, A1) + b2[:, None]\n",
    "            A2 = softmax(Z2)\n",
    "\n",
    "            epsilon = 1e-7  # small constant\n",
    "            A2_clipped = np.clip(A2, epsilon, 1 - epsilon)  # avoid extreme values\n",
    "            loss = -np.mean(y_train_fold * np.log(A2_clipped))\n",
    "            train_losses.append(loss)  # Store the training loss for this epoch\n",
    "\n",
    "            predictions_train = np.argmax(A2, axis=0)\n",
    "            f1 = f1_score(y_train_fold, predictions_train, average=\"macro\")\n",
    "            train_f1_scores.append(f1)  # Store the training F1-score for this epoch\n",
    "\n",
    "            dZ2 = A2 - y_train_fold[None, :]\n",
    "            dW2 = np.dot(dZ2, A1.T) / X_train_fold.shape[0]\n",
    "            db2 = np.sum(dZ2, axis=1) / X_train_fold.shape[0]\n",
    "            dZ1 = np.dot(W2.T, dZ2) * hidden_activation_derivative(Z1, k0, k1)\n",
    "            dW1 = np.dot(dZ1, X_train_fold) / X_train_fold.shape[0]\n",
    "            db1 = np.sum(dZ1, axis=1) / X_train_fold.shape[0]\n",
    "\n",
    "            W1 -= learning_rate * dW1\n",
    "            b1 -= learning_rate * db1\n",
    "            W2 -= learning_rate * dW2\n",
    "            b2 -= learning_rate * db2\n",
    "\n",
    "        Z1 = np.dot(W1, X_val_fold.T) + b1[:, None]\n",
    "        A1 = hidden_activation(Z1, k0, k1)\n",
    "        Z2 = np.dot(W2, A1) + b2[:, None]\n",
    "        A2 = softmax(Z2)\n",
    "        predictions_val = np.argmax(A2, axis=0)\n",
    "\n",
    "        f1_val = f1_score(y_val_fold, predictions_val, average=\"macro\")\n",
    "        accuracy_val = accuracy_score(y_val_fold, predictions_val)\n",
    "\n",
    "        accuracy_train = accuracy_score(y_train_fold, predictions_train)\n",
    "\n",
    "        f1_scores.append(f1_val)\n",
    "        train_accuracies.append(accuracy_train)\n",
    "        test_accuracies.append(accuracy_val)\n",
    "\n",
    "    avg_train_f1 = np.mean(f1_scores)\n",
    "    avg_train_accuracy = np.mean(train_accuracies)\n",
    "    avg_test_f1 = np.mean(f1_scores)\n",
    "    avg_test_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "    avg_train_f1_scores.append(avg_train_f1)\n",
    "\n",
    "    if avg_test_f1 > best_test_f1:\n",
    "        best_train_f1 = avg_train_f1\n",
    "        best_train_accuracy = avg_train_accuracy\n",
    "        best_test_f1 = avg_test_f1\n",
    "        best_test_accuracy = avg_test_accuracy\n",
    "        best_n_h = n_h\n",
    "        best_model = (W1, b1, W2, b2, k0, k1)  # Store the parameters of the best model\n",
    "\n",
    "    print(f\"\\nNumber of nodes: {n_h}\")\n",
    "    print(f\"Train F1-score: {avg_train_f1}\")\n",
    "    print(f\"Train Accuracy: {avg_train_accuracy}\")\n",
    "    print(f\"Validation F1-score: {avg_test_f1}\")\n",
    "    print(f\"Validation Accuracy: {avg_test_accuracy}\")\n",
    "\n",
    "# Print the scores for the best model\n",
    "print(\"\\nBest model:\")\n",
    "print(f\"Number of nodes: {best_n_h}\")\n",
    "print(f\"Best Train F1-score: {best_train_f1}\")\n",
    "print(f\"Best Train Accuracy: {best_train_accuracy}\")\n",
    "print(f\"Best Test F1-score: {best_test_f1}\")\n",
    "print(f\"Best Test Accuracy: {best_test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hidden_layer_sizes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akshat_gosain/Brain_DeepLearning/Mnistd_AkshatGosain_2022A7PS0154G.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnistd_AkshatGosain_2022A7PS0154G.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnistd_AkshatGosain_2022A7PS0154G.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m6\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnistd_AkshatGosain_2022A7PS0154G.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(hidden_layer_sizes, avg_train_f1_scores, marker\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mo\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnistd_AkshatGosain_2022A7PS0154G.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m'\u001b[39m\u001b[39mAverage Training F1-score vs Number of Nodes in the Hidden Layer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnistd_AkshatGosain_2022A7PS0154G.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m plt\u001b[39m.\u001b[39mxlabel(\u001b[39m'\u001b[39m\u001b[39mNumber of Nodes in the Hidden Layer\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hidden_layer_sizes' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hidden_layer_sizes, avg_train_f1_scores, marker='o')\n",
    "plt.title('Average Training F1-score vs Number of Nodes in the Hidden Layer')\n",
    "plt.xlabel('Number of Nodes in the Hidden Layer')\n",
    "plt.ylabel('Average Training F1-score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/akshat_gosain/Brain_DeepLearning/Mnist_AkshatGosain_2022A7PS0154G.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnist_AkshatGosain_2022A7PS0154G.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m train_f1_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnist_AkshatGosain_2022A7PS0154G.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Retrieve the parameters of the best model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnist_AkshatGosain_2022A7PS0154G.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m W1, b1, W2, b2, k0, k1 \u001b[39m=\u001b[39m best_model\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnist_AkshatGosain_2022A7PS0154G.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m20\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/akshat_gosain/Brain_DeepLearning/Mnist_AkshatGosain_2022A7PS0154G.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     Z1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(W1, X_train\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m b1[:, \u001b[39mNone\u001b[39;00m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize lists to store the training loss and F1-score for each epoch\n",
    "train_losses = []\n",
    "train_f1_scores = []\n",
    "\n",
    "# Retrieve the parameters of the best model\n",
    "W1, b1, W2, b2, k0, k1 = best_model\n",
    "\n",
    "for epoch in range(20):\n",
    "    Z1 = np.dot(W1, X_train.T) + b1[:, None]\n",
    "    A1 = hidden_activation(Z1, k0, k1)\n",
    "    Z2 = np.dot(W2, A1) + b2[:, None]\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    epsilon = 1e-7  # small constant\n",
    "    A2_clipped = np.clip(A2, epsilon, 1 - epsilon)  # avoid extreme values\n",
    "    loss = -np.mean(y_train * np.log(A2_clipped))\n",
    "    train_losses.append(loss)  # Store the training loss for this epoch\n",
    "\n",
    "    predictions_train = np.argmax(A2, axis=0)\n",
    "    f1 = f1_score(y_train, predictions_train, average=\"macro\")\n",
    "    train_f1_scores.append(f1)  # Store the training F1-score for this epoch\n",
    "\n",
    "    dZ2 = A2 - y_train[None, :]\n",
    "    dW2 = np.dot(dZ2, A1.T) / X_train.shape[0]\n",
    "    db2 = np.sum(dZ2, axis=1) / X_train.shape[0]\n",
    "    dZ1 = np.dot(W2.T, dZ2) * hidden_activation_derivative(Z1, k0, k1)\n",
    "    dW1 = np.dot(dZ1, X_train) / X_train.shape[0]\n",
    "    db1 = np.sum(dZ1, axis=1) / X_train.shape[0]\n",
    "\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "# Test the model on the test data\n",
    "Z1 = np.dot(W1, X_test.T) + b1[:, None]\n",
    "A1 = hidden_activation(Z1, k0, k1)\n",
    "Z2 = np.dot(W2, A1) + b2[:, None]\n",
    "A2 = softmax(Z2)\n",
    "predictions_test = np.argmax(A2, axis=0)\n",
    "\n",
    "f1_test = f1_score(y_test, predictions_test, average=\"macro\")\n",
    "accuracy_test = accuracy_score(y_test, predictions_test)\n",
    "\n",
    "print(\"\\nBest model:\")\n",
    "print(f\"Number of nodes: {best_n_h}\")\n",
    "print(f\"Train F1-score: {best_train_f1}\")\n",
    "print(f\"Train Accuracy: {best_train_accuracy}\")\n",
    "print(f\"Test F1-score: {f1_test}\")\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n",
    "\n",
    "# Plot the training loss and F1-score for each epoch\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 1001), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, 1001), train_f1_scores, label='Train F1-score')\n",
    "plt.title('Training Loss and Train F1-score vs number of epochs')\n",
    "plt.xlabel('Number of epochs')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
